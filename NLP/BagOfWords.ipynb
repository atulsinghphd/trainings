{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #99CD4E; text-align:center; vertical-align: middle; padding:40px 0;\"> \n",
    "  <h1 style=\"color: white;\"> *Vector representation of text documents* </h1>.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #99CD4E; padding:5px 0;\"> \n",
    "  <h2 style=\"color: white;\"> Why do we need this ? </h1>\n",
    " </div>\n",
    "- Machine learning algorithms work with numbers \n",
    "- Apply other algorithms\n",
    "\n",
    "See article [An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #99CD4E; padding:5px 0;\"> \n",
    "  <h2 style=\"color: white;\"> *Bag of words & Vector Space Model* </h1>\n",
    " </div>\n",
    "- *BAG OF WORDS*: The exact ordering of the terms in a document is ignored but the number of occurences of each term is material\n",
    "\n",
    "- *VECTOR SPACE MODEL*: Representation of a set of documents as vectors in a common vector space\n",
    "    - The dimensions of the vectore space are the words in the dictionary, and the value for each dimension is a weight that is indicative of the occurence of the word in the document. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #99CD4E; padding:5px 0;\"> \n",
    "  <h2 style=\"color: white;\"> *Term frequency-inverse document frequency (Tf-idf)* </h1>\n",
    "</div>\n",
    "\n",
    "_**Term frequency (tf<sub>t,d</sub>)**_ Number of occurences of a term(word) t in a document d \n",
    "\n",
    "_**Inverse document frequency (idf<sub>t</sub>)**_ Document frequency df<sub>t</sub> of a term t is the number of documents that contain a term. Let N be number of documents in collection. Inverse document frequency is $$ idf_{t} = log(\\frac{N}{df_{t}})$$\n",
    "\n",
    "_**Tf<sub>t,d</sub>-idf<sub>t</sub>**_ of a term is product of term frequency (tf<sub>t,d</sub>) and inverse document frequency idf<sub>t</sub>). $$ Tf_{t,d}-idf_{t} = Tf_{t,d} \\ast idf_{t} $$ \n",
    "\n",
    "It has the property:\n",
    "\n",
    "    1. higher when word occurs many times within small number of documents\n",
    "    2. lower when word occurs fewer times in a document, or occurs in many documents\n",
    "    3. lowest when term occurs in virtuall all documents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a collection of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_collection = [ 'A group of kids is playing in a yard and an old man is standing in the background',\n",
    "                'A group of children is playing in the house and there is no man standing in the background',\n",
    "                'The young boys are playing outdoors and the man is smiling nearby',\n",
    "                'The kids are playing outdoors near a man with a smile',\n",
    "                'There is no boy playing outdoors and there is no man smiling',\n",
    "                'A group of boys in a yard is playing and a man is standing in the background',\n",
    "                'A brown dog is attacking another animal in front of the tall man in pants',\n",
    "                'A brown dog is attacking another dog in front of the man in pants',\n",
    "                'Two dogs are fighting',\n",
    "                'Two dogs are wrestling and hugging']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for sent in document_collection:\n",
    "    tokens = tokens + word_tokenize(sent)\n",
    "\n",
    "fd = nltk.FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 11), ('in', 10), ('man', 8), ('the', 7), ('and', 6)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "[Spacy](https://spacy.io) is another popular Python natural language processing package. Write the code to count word frequencies using Spcay?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency<sub>t,d</sub> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "term_frequency_vectorizer = CountVectorizer(stop_words='english')\n",
    "term_frequency_model = term_frequency_vectorizer.fit_transform(document_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'animal': 0,\n",
       " 'attacking': 1,\n",
       " 'background': 2,\n",
       " 'boy': 3,\n",
       " 'boys': 4,\n",
       " 'brown': 5,\n",
       " 'children': 6,\n",
       " 'dog': 7,\n",
       " 'dogs': 8,\n",
       " 'fighting': 9,\n",
       " 'group': 10,\n",
       " 'house': 11,\n",
       " 'hugging': 12,\n",
       " 'kids': 13,\n",
       " 'man': 14,\n",
       " 'near': 15,\n",
       " 'nearby': 16,\n",
       " 'old': 17,\n",
       " 'outdoors': 18,\n",
       " 'pants': 19,\n",
       " 'playing': 20,\n",
       " 'smile': 21,\n",
       " 'smiling': 22,\n",
       " 'standing': 23,\n",
       " 'tall': 24,\n",
       " 'wrestling': 25,\n",
       " 'yard': 26,\n",
       " 'young': 27}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_frequency_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['animal', 'attacking', 'background', 'boy', 'boys', 'brown', 'children', 'dog', 'dogs', 'fighting', 'group', 'house', 'hugging', 'kids', 'man', 'near', 'nearby', 'old', 'outdoors', 'pants', 'playing', 'smile', 'smiling', 'standing', 'tall', 'wrestling', 'yard', 'young']\n"
     ]
    }
   ],
   "source": [
    "terms = term_frequency_vectorizer.get_feature_names()\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.shape(term_frequency_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(term_frequency_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "term_frequency_model_as_dataframe = pd.DataFrame(term_frequency_model.toarray(), columns= terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal</th>\n",
       "      <th>attacking</th>\n",
       "      <th>background</th>\n",
       "      <th>boy</th>\n",
       "      <th>boys</th>\n",
       "      <th>brown</th>\n",
       "      <th>children</th>\n",
       "      <th>dog</th>\n",
       "      <th>dogs</th>\n",
       "      <th>fighting</th>\n",
       "      <th>group</th>\n",
       "      <th>house</th>\n",
       "      <th>hugging</th>\n",
       "      <th>kids</th>\n",
       "      <th>man</th>\n",
       "      <th>near</th>\n",
       "      <th>nearby</th>\n",
       "      <th>old</th>\n",
       "      <th>outdoors</th>\n",
       "      <th>pants</th>\n",
       "      <th>playing</th>\n",
       "      <th>smile</th>\n",
       "      <th>smiling</th>\n",
       "      <th>standing</th>\n",
       "      <th>tall</th>\n",
       "      <th>wrestling</th>\n",
       "      <th>yard</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   animal  attacking  background  boy  boys  brown  children  dog  dogs  \\\n",
       "0       0          0           1    0     0      0         0    0     0   \n",
       "1       0          0           1    0     0      0         1    0     0   \n",
       "2       0          0           0    0     1      0         0    0     0   \n",
       "3       0          0           0    0     0      0         0    0     0   \n",
       "4       0          0           0    1     0      0         0    0     0   \n",
       "\n",
       "   fighting  group  house  hugging  kids  man  near  nearby  old  outdoors  \\\n",
       "0         0      1      0        0     1    1     0       0    1         0   \n",
       "1         0      1      1        0     0    1     0       0    0         0   \n",
       "2         0      0      0        0     0    1     0       1    0         1   \n",
       "3         0      0      0        0     1    1     1       0    0         1   \n",
       "4         0      0      0        0     0    1     0       0    0         1   \n",
       "\n",
       "   pants  playing  smile  smiling  standing  tall  wrestling  yard  young  \n",
       "0      0        1      0        0         1     0          0     1      0  \n",
       "1      0        1      0        0         1     0          0     0      0  \n",
       "2      0        1      0        1         0     0          0     0      1  \n",
       "3      0        1      1        0         0     0          0     0      0  \n",
       "4      0        1      0        1         0     0          0     0      0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_columns', None)\n",
    "term_frequency_model_as_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_model = tfidf_vectorizer.fit_transform(document_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['animal', 'attacking', 'background', 'boy', 'boys', 'brown', 'children', 'dog', 'dogs', 'fighting', 'group', 'house', 'hugging', 'kids', 'man', 'near', 'nearby', 'old', 'outdoors', 'pants', 'playing', 'smile', 'smiling', 'standing', 'tall', 'wrestling', 'yard', 'young']\n"
     ]
    }
   ],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.shape(tfidf_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tfidf_model_as_dataframe = pd.DataFrame(tfidf_model.toarray(), columns=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal</th>\n",
       "      <th>attacking</th>\n",
       "      <th>background</th>\n",
       "      <th>boy</th>\n",
       "      <th>boys</th>\n",
       "      <th>brown</th>\n",
       "      <th>children</th>\n",
       "      <th>dog</th>\n",
       "      <th>dogs</th>\n",
       "      <th>fighting</th>\n",
       "      <th>group</th>\n",
       "      <th>house</th>\n",
       "      <th>hugging</th>\n",
       "      <th>kids</th>\n",
       "      <th>man</th>\n",
       "      <th>near</th>\n",
       "      <th>nearby</th>\n",
       "      <th>old</th>\n",
       "      <th>outdoors</th>\n",
       "      <th>pants</th>\n",
       "      <th>playing</th>\n",
       "      <th>smile</th>\n",
       "      <th>smiling</th>\n",
       "      <th>standing</th>\n",
       "      <th>tall</th>\n",
       "      <th>wrestling</th>\n",
       "      <th>yard</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396791</td>\n",
       "      <td>0.207202</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250571</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396791</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.491198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365318</td>\n",
       "      <td>0.491198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218049</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263689</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.401465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209643</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.472261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253523</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.401465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438341</td>\n",
       "      <td>0.228899</td>\n",
       "      <td>0.51564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276810</td>\n",
       "      <td>0.51564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.601817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267154</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.323072</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.511599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234289</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283329</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448664</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.443343</td>\n",
       "      <td>0.376882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.376882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.376882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196805</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.376882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.443343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.741622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193635</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.647689</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     animal  attacking  background       boy      boys     brown  children  \\\n",
       "0  0.000000   0.000000    0.347145  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000   0.000000    0.365318  0.000000  0.000000  0.000000  0.491198   \n",
       "2  0.000000   0.000000    0.000000  0.000000  0.401465  0.000000  0.000000   \n",
       "3  0.000000   0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000   0.000000    0.000000  0.601817  0.000000  0.000000  0.000000   \n",
       "5  0.000000   0.000000    0.392528  0.000000  0.448664  0.000000  0.000000   \n",
       "6  0.443343   0.376882    0.000000  0.000000  0.000000  0.376882  0.000000   \n",
       "7  0.000000   0.370811    0.000000  0.000000  0.000000  0.370811  0.000000   \n",
       "8  0.000000   0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.000000   0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        dog      dogs  fighting     group     house   hugging      kids  \\\n",
       "0  0.000000  0.000000  0.000000  0.347145  0.000000  0.000000  0.396791   \n",
       "1  0.000000  0.000000  0.000000  0.365318  0.491198  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.438341   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.000000  0.000000  0.392528  0.000000  0.000000  0.000000   \n",
       "6  0.376882  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.741622  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.000000  0.647689  0.761905  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.000000  0.515192  0.000000  0.000000  0.000000  0.606043  0.000000   \n",
       "\n",
       "        man     near    nearby       old  outdoors     pants   playing  \\\n",
       "0  0.207202  0.00000  0.000000  0.466762  0.000000  0.000000  0.250571   \n",
       "1  0.218049  0.00000  0.000000  0.000000  0.000000  0.000000  0.263689   \n",
       "2  0.209643  0.00000  0.472261  0.000000  0.351235  0.000000  0.253523   \n",
       "3  0.228899  0.51564  0.000000  0.000000  0.383497  0.000000  0.276810   \n",
       "4  0.267154  0.00000  0.000000  0.000000  0.447589  0.000000  0.323072   \n",
       "5  0.234289  0.00000  0.000000  0.000000  0.000000  0.000000  0.283329   \n",
       "6  0.196805  0.00000  0.000000  0.000000  0.000000  0.376882  0.000000   \n",
       "7  0.193635  0.00000  0.000000  0.000000  0.000000  0.370811  0.000000   \n",
       "8  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "     smile   smiling  standing      tall  wrestling      yard     young  \n",
       "0  0.00000  0.000000  0.347145  0.000000   0.000000  0.396791  0.000000  \n",
       "1  0.00000  0.000000  0.365318  0.000000   0.000000  0.000000  0.000000  \n",
       "2  0.00000  0.401465  0.000000  0.000000   0.000000  0.000000  0.472261  \n",
       "3  0.51564  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  \n",
       "4  0.00000  0.511599  0.000000  0.000000   0.000000  0.000000  0.000000  \n",
       "5  0.00000  0.000000  0.392528  0.000000   0.000000  0.448664  0.000000  \n",
       "6  0.00000  0.000000  0.000000  0.443343   0.000000  0.000000  0.000000  \n",
       "7  0.00000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  \n",
       "8  0.00000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  \n",
       "9  0.00000  0.000000  0.000000  0.000000   0.606043  0.000000  0.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_model_as_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given a new sentence: \"Two dogs are playing in the yard\", write the code in the cell below to get the tfidf representation of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hint use the function transform() on the tfidf vectorizer\n",
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Does the code above converts the text into lower case ? If not then how can you do the same ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #99CD4E; padding:5px 0;\"> \n",
    "  <h2 style=\"color: white;\"> Stemming and lemmatization </h2>\n",
    "</div>\n",
    "\n",
    "Refer [Stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "A word can have multiple forms. For example:\n",
    "- organize, organizes, and organizing (different forms of one word)\n",
    "- democracy, democratic, and democratization (related words with different meaning)\n",
    "\n",
    "Both stemming and lemmatization reduce a word to a common base form.  \n",
    "\n",
    "~~~~\n",
    "Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. \n",
    "~~~~\n",
    "\n",
    "~~~~\n",
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma \n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list1 = ['organize', 'organizes', 'organizing']\n",
    "word_list2 = ['democracy', 'democratic', 'democratization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing word list 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_list1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dbaa52cf1515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processing word list 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordnet_lemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_list1' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print('Processing word list 1')\n",
    "for word in word_list1:\n",
    "    print(wordnet_lemmatizer.lemmatize(word, 'v'))\n",
    "\n",
    "print('Processing word list 2')\n",
    "for word in word_list2:\n",
    "    print(wordnet_lemmatizer.lemmatize(word, 'n'))\n",
    "    print(wordnet_lemmatizer.lemmatize(word, 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing word list 1\n",
      "organ\n",
      "organ\n",
      "organ\n",
      "Processing word list 2\n",
      "democraci\n",
      "democrat\n",
      "democrat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "print('Processing word list 1')\n",
    "for word in word_list1:\n",
    "    print(stemmer.stem(word))\n",
    "\n",
    "print('Processing word list 2')\n",
    "for word in word_list2:\n",
    "    print(stemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "\n",
    "- How can you incorporate lemmatization in the process of generating TFIDF representation ? Hint: Refer [Document Clustering with Python](http://brandonrose.org/clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #99CD4E; text-align:center; vertical-align: middle; padding:40px 0;\"> \n",
    "  <h1 style=\"color: white;\"> *The End* </h1>.\n",
    " </div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
