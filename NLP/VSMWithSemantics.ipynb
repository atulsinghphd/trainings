{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #99CD4E; text-align:center; vertical-align: middle; padding:40px 0;\"> \n",
    "  <h1 style=\"color: white;\"> *Incorporating semantics in Vector Space Models (VSM)* </h1>.\n",
    " </div>\n",
    " \n",
    " Word vectors are called word embedings\n",
    " \n",
    " # References\n",
    " - [Natural Language Processing with Deep Learning (Stanford University) Lecture 2](http://web.stanford.edu/class/cs224n/lectures/lecture2.pdf)\n",
    " - [An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)\n",
    " - [Vector Representations of Words](https://www.tensorflow.org/tutorials/word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #99CD4E; padding:5px 0;\"> \n",
    "  <h2 style=\"color: white;\"> Challenges </h1>\n",
    " </div>\n",
    " \n",
    "- Thesaurus or dictionary based approach like wordnet do not capture the nuances of words, have to be manually created and updated.\n",
    "- Count based approaches do not have a notion of similarity of words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #99CD4E; padding:5px 0;\"> \n",
    "  <h2 style=\"color: white;\"> Distributional Semantics </h1>\n",
    " </div>\n",
    " \n",
    " A word’s meaning is given by the words that frequently appear close-by\n",
    " \n",
    " > “You shall know a word by the company it keeps” (J. R. Firth 1957: 11)\n",
    " \n",
    "- When a word w appears in a text, its context is the set of words that appear nearby (within a fixed-size window).\n",
    "- Use the many contexts of w to build up a representation of w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #99CD4E; padding:5px 0;\"> \n",
    "  <h2 style=\"color: white;\"> Taxonomy of distributional approaches </h1>\n",
    " </div>\n",
    " \n",
    " - Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word. e.g., [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "    - Dimensionality reduction technique like Principal Component Analysis (PCA) and t-SNE can be used\n",
    " \n",
    " \n",
    " - Predictive models directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors (considered parameters of the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #99CD4E; padding:5px 0;\"> \n",
    "  <h2 style=\"color: white;\"> Word2Vec </h1>\n",
    " </div>\n",
    "\n",
    "Word2vec [Mikolov et al. 2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) is a framework for learning word vectors. Idea:\n",
    "- We have a large corpus of text\n",
    "- Every word in a fixed vocabulary is represented by a vector\n",
    "- Go through each position t in the text, which has a center word c and context (“outside”) words o\n",
    "- Use the similarity of the word vectors for c and o to calculate the probability of o given c (or vice versa)\n",
    "- Keep adjusting the word vectors to maximize this probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec model using Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "books = ['shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.util.LazyConcatenation"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = gutenberg.sents('shakespeare-caesar.txt') + gutenberg.sents('shakespeare-hamlet.txt') + gutenberg.sents('shakespeare-macbeth.txt')\n",
    "type(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bard = Word2Vec(sents, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1588"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bard.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get vector of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10742793, -0.01956181,  0.13975601,  0.07485832, -0.17751361,\n",
       "       -0.06080489,  0.1121189 , -0.04487086, -0.24221551, -0.14671983,\n",
       "        0.11934487,  0.02752467, -0.35037538, -0.17839876, -0.36048526,\n",
       "       -0.00681653, -0.2306423 , -0.06548589,  0.37666312,  0.08077748,\n",
       "       -0.34400108, -0.32902193, -0.3022477 ,  0.11491542, -0.2893151 ,\n",
       "        0.21328667, -0.26541919,  0.56787801, -0.06792197, -0.05740376,\n",
       "        0.13355957, -0.45053831,  0.18514419, -0.31926861, -0.1611011 ,\n",
       "        0.10380171,  0.24726281, -0.19478726, -0.18347853,  0.31361559,\n",
       "        0.26796308, -0.12502605,  0.11905139, -0.41982856, -0.35766694,\n",
       "       -0.02638668,  0.09754109,  0.22023383, -0.05532344, -0.09578989,\n",
       "       -0.06329821,  0.13999194,  0.1025404 , -0.1021321 ,  0.01795489,\n",
       "        0.15535243,  0.04837716,  0.10837361, -0.32653791, -0.10438953,\n",
       "       -0.04949269,  0.48600715, -0.57529658,  0.07215577, -0.18409459,\n",
       "        0.54216701, -0.05354522,  0.01323699, -0.4829801 ,  0.19136097,\n",
       "        0.1783209 ,  0.0626976 ,  0.03385507, -0.33430377,  0.15723975,\n",
       "        0.1231757 , -0.03188631,  0.34454188,  0.19959007, -0.01616273,\n",
       "       -0.38243258, -0.0678421 ,  0.09795678, -0.19637229,  0.16115439,\n",
       "        0.00719003, -0.18030226,  0.15182003,  0.22431299, -0.46717867,\n",
       "       -0.02679464,  0.09446043, -0.09752527, -0.0590119 ,  0.10604181,\n",
       "       -0.1692339 ,  0.5042451 , -0.49437761,  0.14496079,  0.08893321], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bard.wv['Macbeth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "- How will you get the vector representation of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hamlet', 0.9997541904449463),\n",
       " ('Antony', 0.999751091003418),\n",
       " ('Queene', 0.9997130632400513),\n",
       " ('Banquo', 0.9997089505195618),\n",
       " ('Horatio', 0.9997077584266663),\n",
       " ('Oh', 0.999681293964386),\n",
       " ('&', 0.9996806979179382),\n",
       " ('Cassius', 0.999677836894989),\n",
       " ('In', 0.9996768832206726),\n",
       " ('three', 0.999654233455658)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bard.most_similar('Macbeth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 0.17482200264930725),\n",
       " ('Len', 0.14525464177131653),\n",
       " ('Messa', 0.0918571725487709),\n",
       " ('Ment', 0.09179973602294922),\n",
       " ('Cin', 0.08558960258960724),\n",
       " ('Osr', 0.08130502700805664),\n",
       " ('Tit', 0.06718209385871887),\n",
       " ('Hora', 0.06625272333621979),\n",
       " ('Cask', 0.060132596641778946),\n",
       " ('Murth', 0.04870552569627762)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bard.wv.most_similar(positive=['Macbeth'], negative=['woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #99CD4E; text-align:center; vertical-align: middle; padding:40px 0;\"> \n",
    "  <h1 style=\"color: white;\"> *The End* </h1>.\n",
    " </div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
